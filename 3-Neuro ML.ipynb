{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconvolving spikes from Ca2+ imaging\n",
    "\n",
    "### Short intro to the experimental technique\n",
    "\n",
    "A common way of measuring neural activity (spiking) is with calcium (Ca2+) indicators. These are designer proteins that are genetically inserted into all neurons or specific types of neurons. The protein has a part that senses calcium ions and another part that fluoresces when the sensor is activated.\n",
    "\n",
    "Spiking in the neuron is an electrical phenomenon: the spike is a sharp rise in the voltage across the neuron's membrane. But it is also a chemical phenomenon: charged ions (principally Na+, K+, Cl-, and Ca2+) flowing in and out of the cell lead to this change. After a spike, Ca2+ rises sharply in the cell before decaying back to its baseline concentration.\n",
    "\n",
    "Spiking is a fast process, happening on the order of milliseconds. Calcium transients following spiking is 1-2 orders of magnitude slower, occuring on 10-100 ms timescales. This is compounded by the effects of the Ca2+ binding to the sensor part of the indicator protein.\n",
    "\n",
    "So in reality, the calcium signal we can measure with this technique is a smoothed out version of the underlying fast and spiky neuron firing.\n",
    "\n",
    "### Mathematizing things\n",
    "\n",
    "The neuron spiking (unknown) and Ca2+ measurements (known) are represented as two timeseries $s(t)$ and $x(t)$.\n",
    "So the **inverse problem** of spike deconvolution is to estimate $s(t)$ given $x(t)$.\n",
    "\n",
    "Since spiking is *fast and short-lived*, the spiking behavior will look something like:\n",
    "\n",
    "$$\n",
    "s(t) = (0, 0, 1, 0, 0, 0, 0, 0,\\ldots)\n",
    "$$\n",
    "\n",
    "whereas the calcium signal that we can measure would look like \n",
    "\n",
    "$$\n",
    "x(t) = (0, 0, 0.9, 0.35, 0.3, 0.2, 0.1, 0.05, \\ldots) .\n",
    "$$\n",
    "\n",
    "The spike leads to a rapid increase but slow decay. \n",
    "(In reality, there is some finite rise time as well, but let's ignore that for simplicity.)\n",
    "\n",
    "What is lacking so far is a description of the forward model $f: s(t) \\to x(t)$.\n",
    "A simple one is autoregression:\n",
    "\n",
    "$$\n",
    "x(t + 1) = \\gamma x(t) + s(t+1) + \\xi(t+1), \\qquad \\text{(AR)}\n",
    "$$\n",
    "where $\\xi(t) \\sim \\mathcal{N}(0,\\sigma^2)$ is a Gaussian noise term.\n",
    "\n",
    "This AR(1) model has one free parameter, $\\gamma$, along with the noise variance $\\sigma^2$.\n",
    "\n",
    "## Exercise 1 (pen & paper)\n",
    "For this exercise, assume $\\sigma = 0$.\n",
    "\n",
    "* Assume $s(1) = 1$ and $s(t > 1) = 0$. Come up with a formula for $x(t)$ given the AR model for arbitrary $t = 1, 2, 3,\\ldots$. \n",
    "* What kind of function is this? \n",
    "* What is the condition on $\\gamma$ for the solution to be *stable* (not blow up as $t \\to \\infty$)?\n",
    "\n",
    "## Exercise 2 (pen & paper)\n",
    "\n",
    "Now we want to implement such a model on the computer. We will model a set of $T$ total timepoints\n",
    "$t = 1, 2, \\ldots, T$. In this case, our timeseries $x(t)$ and $s(t)$ are just vectors of length $T$ which we will denote as $\\bf{x}$ and $\\bf{s}$.\n",
    "\n",
    "* Write a matrix $\\bf{A}$ that represents the AR model. It should satisfy $\\bf{x} = \\bf{A} \\bf{s}$. For boundary conditions assume there are no spikes before $t=1$. Ignore the noise term.\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "Below we have some code that will generate data from the AR model. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(2)\n",
    "\n",
    "T = 500\n",
    "gamma = 0.9\n",
    "s = np.array(np.random.rand(T,) > 0.96)\n",
    "\n",
    "def gen_AR_data(s, gamma = 0.9, noise = 0.1):\n",
    "    x = np.zeros(len(s),)\n",
    "    for t in range(len(s)):\n",
    "        if t > 1:\n",
    "            x[t] = gamma * x[t-1] + s[t]\n",
    "        else:\n",
    "            x[t] = s[t]\n",
    "    x = x + np.random.randn(len(s),) * noise\n",
    "    return x\n",
    "\n",
    "x = gen_AR_data(s, gamma)\n",
    "\n",
    "plt.plot(s)\n",
    "plt.plot(x)\n",
    "plt.legend(['spikes', 'Ca2+'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that we have some data, make the matrix $\\bf{A}$ below. There is some code to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((T, T))\n",
    "for j in range(T):\n",
    "    for i in range(j, T):\n",
    "        A[i,j] = ## fill this in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that your matrix works, make a plot of ${\\bf A s}$ and $\\bf{x}$. They should look the same, up to the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(A @ s)\n",
    "plt.plot(x)\n",
    "plt.legend(['A s', 'x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: inverse problem\n",
    "\n",
    "Now, to recover the spikes, we have to solve\n",
    "$\n",
    "\\bf x = A s\n",
    "$\n",
    "for $\\bf s$.\n",
    "\n",
    "* Solve the equation. You may want to use `np.linalg.inv` or `np.linalg.pinv`. Call your solution `shat`.\n",
    "* Plot the solution `shat` as well as the truth `s`.\n",
    "* What do you observe about the solution? Is it good or bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: enforcing sparsity (advanced)\n",
    "\n",
    "We have some *prior* information about the spike train $\\bf s$: it is *sparse*.\n",
    "\n",
    "One way to force the solution to be sparse is to penalize the nonzero entries in the vector somehow.\n",
    "A popular approach is LASSO. \n",
    "\n",
    "$$\n",
    "\\hat{\\bf s} = \\arg \\min_{\\bf s} \\| {\\bf x - A s} \\|_2^2 + \\alpha \\| {\\bf s} \\|_1\n",
    "$$\n",
    "\n",
    "It has a tuning parameter $\\alpha$. A larger $\\alpha$ leads to a sparser solution.\n",
    "\n",
    "It is implemented in `scikit-learn` but has a sort of weird syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "estimator = Lasso(fit_intercept = False, alpha=1e-3)\n",
    "\n",
    "# first you have to solve the optimization problem by calling the 'fit' function\n",
    "estimator.fit(A, x)\n",
    "# now that it's been fit, the solution is stored in the 'coef_' argument\n",
    "shat2 = estimator.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the residual $\\bf{ r = \\hat{s} - s}$ from before and the new residual $\\bf{ r = \\hat{s}_2 - s}$. Is your solution any better than before?\n",
    "* Plot $\\bf \\hat{s}$ and $\\bf s$ for just the first 100 time points. Where does it make the mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning\n",
    "\n",
    "It turns out the LASSO solution depends strongly on the parameter $\\alpha$. Experiment with this using the code in the previous cells and see what happens when $\\alpha$ is much larger or much smaller than `1e-3`. It is usually best to vary such *regularization parameters* on a logarithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
